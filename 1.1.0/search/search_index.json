{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Architecture Overview Introduction Usage Scenarios Architecture Features Introduction Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases. Usage Scenarios Usage Scenario 1 -- Interactive queries Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude. Usage Scenario 2 -- Batch processing jobs Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need. Architecture The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations. Features Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs. Indexing Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution. Caching Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. *Other names and brands may be claimed as the property of others.","title":"Architecture Overview"},{"location":"#architecture-overview","text":"Introduction Usage Scenarios Architecture Features","title":"Architecture Overview"},{"location":"#introduction","text":"Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.","title":"Introduction"},{"location":"#usage-scenarios","text":"","title":"Usage Scenarios"},{"location":"#usage-scenario-1-interactive-queries","text":"Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.","title":"Usage Scenario 1 -- Interactive queries"},{"location":"#usage-scenario-2-batch-processing-jobs","text":"Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need.","title":"Usage Scenario 2 -- Batch processing jobs"},{"location":"#architecture","text":"The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.","title":"Architecture"},{"location":"#features","text":"Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.","title":"Features"},{"location":"#indexing","text":"Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.","title":"Indexing"},{"location":"#caching","text":"Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.","title":"Caching"},{"location":"#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"Advanced-Configuration/","text":"Advanced Configuration In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section. Their needed dependencies like Memkind , Vmemcache and Plasma can be automatically installed when following OAP Installation Guide , corresponding feature jars can be found under $HOME/miniconda2/envs/oapenv/oap_jars . Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. Additional Cache Strategies Following table shows features of 4 cache strategies on PMem. guava noevict vmemcache external cache Use memkind lib to operate on PMem and guava cache strategy when data eviction happens. Use memkind lib to operate on PMem and doesn't allow data eviction. Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens. Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Doesn't need numa patch. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Node-level cache so there are no limitation for executor number. Cache data cleaned once executors exited. Cache data cleaned once executors exited. Cache data cleaned once executors exited. No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions. For cache solution guava/noevict , make sure Memkind library installed on every cluster worker node. If you have finished OAP Installation Guide , libmemkind will be installed. Or manually build and install it following memkind-installation , then place libmemkind.so.0 under /lib64/ on each worker node. For cache solution vmemcahe/external cache, make sure Vmemcache library has been installed on every cluster worker node. If you have finished OAP Installation Guide , libvmemcache will be installed. Or you can follow the vmemcache-installation steps and make sure libvmemcache.so.0 exist under /lib64/ directory on each worker node. If you have followed OAP Installation Guide , Memkind , Vmemcache and Plasma will be automatically installed. Or you can refer to OAP Developer-Guide , there is a shell script to help you install these dependencies automatically. Use PMem Cache Prerequisites The following are required to configure OAP to use PMem cache. PMem hardware is successfully deployed on each node in cluster. Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in User-Guide : # destroy interleaved PMem device which you set when using external cache strategy umount /mnt/pmem dmsetup remove striped-pmem echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes. For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory Configuration for NUMA Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark . Configuration for PMem Create persistent-memory.xml under $SPARK_HOME/conf if it doesn't exist. Use the following template and change the initialPath to your mounted paths for PMem devices. <persistentMemoryPool> <!--The numa id--> <numanode id=\"0\"> <!--The initial path for Intel Optane DC persistent memory--> <initialPath>/mnt/pmem0</initialPath> </numanode> <numanode id=\"1\"> <initialPath>/mnt/pmem1</initialPath> </numanode> </persistentMemoryPool> Guava cache Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below. NOTE : spark.executor.sql.oap.cache.persistent.memory.reserved.size : When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.sql.oap.cache.memory.manager pm spark.oap.cache.strategy guava # PMem capacity per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # enable SQL Index and Data Source Cache jar in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Memkind library also support DAX KMEM mode. Refer to Kernel , this chapter will guide how to configure PMem as system RAM. Or Memkind support for KMEM DAX option for more details. Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from pm to kmem as below. spark.sql.oap.cache.memory.manager kmem Noevict cache The noevict cache strategy is also supported in OAP based on the memkind library for PMem. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.oap.cache.strategy noevict spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Vmemcache Make sure Vmemcache library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished OAP-Installation-Guide , vmemcache library will be automatically installed by Conda. Or you can follow the build/install steps and make sure libvmemcache.so exist in /lib64 directory on each worker node. - To use it in your workload, follow prerequisites to set up PMem hardware correctly. Configure to enable PMem cache Make the following configuration changes in $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable vmemcache strategy spark.oap.cache.strategy vmem spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # according to your cluster spark.executor.sql.oap.cache.guardian.memory.size 10g The vmem cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache. spark.executor.instances : We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket. spark.executor.sql.oap.cache.persistent.memory.initial.size : It is configured to the available PMem capacity to be used as data cache per exectutor. NOTE : If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set spark.executor.sql.oap.cache.guardian.memory.size to a larger number as the default size is 10GB. The user could also increase spark.sql.oap.cache.guardian.free.thread.nums or decrease spark.sql.oap.cache.dispose.timeout.ms to free memory more quickly. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Verify NUMA binding status by confirming keywords like numactl --cpubind=1 --membind=1 contained in executor launch command. Check PMem cache size by checking disk space with df -h .For vmemcache strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For Guava/Noevict strategies, the command will show disk space usage increases along with workload execution. Index and Data Cache Separation SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio. Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to spark-defaults.conf . 1. DRAM as cache media, guava strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager offheap The rest configuration you can refer to Use DRAM Cache PMem as cache media, external strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager tmp spark.sql.oap.mix.data.cache.backend external spark.sql.oap.mix.index.cache.backend external The rest configurations can refer to the configurations of PMem Cache and External cache DRAM( offheap )/ guava as index cache media and backend, PMem( tmp )/ external as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix spark.sql.oap.mix.data.cache.backend external # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.memory.offHeap.enabled false spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true DRAM( offheap )/ guava as index cache media and backend, PMem( pm )/ guava as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 spark.memory.offHeap.enabled false # PMem capacity per executor spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true Cache Hot Tables Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2> Column Vector Cache This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true Large Scale and Heterogeneous Cluster Support NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Advanced Configuration"},{"location":"Advanced-Configuration/#advanced-configuration","text":"In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section. Their needed dependencies like Memkind , Vmemcache and Plasma can be automatically installed when following OAP Installation Guide , corresponding feature jars can be found under $HOME/miniconda2/envs/oapenv/oap_jars . Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.","title":"Advanced Configuration"},{"location":"Advanced-Configuration/#additional-cache-strategies","text":"Following table shows features of 4 cache strategies on PMem. guava noevict vmemcache external cache Use memkind lib to operate on PMem and guava cache strategy when data eviction happens. Use memkind lib to operate on PMem and doesn't allow data eviction. Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens. Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Doesn't need numa patch. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Node-level cache so there are no limitation for executor number. Cache data cleaned once executors exited. Cache data cleaned once executors exited. Cache data cleaned once executors exited. No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions. For cache solution guava/noevict , make sure Memkind library installed on every cluster worker node. If you have finished OAP Installation Guide , libmemkind will be installed. Or manually build and install it following memkind-installation , then place libmemkind.so.0 under /lib64/ on each worker node. For cache solution vmemcahe/external cache, make sure Vmemcache library has been installed on every cluster worker node. If you have finished OAP Installation Guide , libvmemcache will be installed. Or you can follow the vmemcache-installation steps and make sure libvmemcache.so.0 exist under /lib64/ directory on each worker node. If you have followed OAP Installation Guide , Memkind , Vmemcache and Plasma will be automatically installed. Or you can refer to OAP Developer-Guide , there is a shell script to help you install these dependencies automatically.","title":"Additional Cache Strategies"},{"location":"Advanced-Configuration/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"Advanced-Configuration/#prerequisites","text":"The following are required to configure OAP to use PMem cache. PMem hardware is successfully deployed on each node in cluster. Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in User-Guide : # destroy interleaved PMem device which you set when using external cache strategy umount /mnt/pmem dmsetup remove striped-pmem echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes. For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory","title":"Prerequisites"},{"location":"Advanced-Configuration/#configuration-for-numa","text":"Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark .","title":"Configuration for NUMA"},{"location":"Advanced-Configuration/#configuration-for-pmem","text":"Create persistent-memory.xml under $SPARK_HOME/conf if it doesn't exist. Use the following template and change the initialPath to your mounted paths for PMem devices. <persistentMemoryPool> <!--The numa id--> <numanode id=\"0\"> <!--The initial path for Intel Optane DC persistent memory--> <initialPath>/mnt/pmem0</initialPath> </numanode> <numanode id=\"1\"> <initialPath>/mnt/pmem1</initialPath> </numanode> </persistentMemoryPool>","title":"Configuration for PMem"},{"location":"Advanced-Configuration/#guava-cache","text":"Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below. NOTE : spark.executor.sql.oap.cache.persistent.memory.reserved.size : When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.sql.oap.cache.memory.manager pm spark.oap.cache.strategy guava # PMem capacity per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # enable SQL Index and Data Source Cache jar in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Memkind library also support DAX KMEM mode. Refer to Kernel , this chapter will guide how to configure PMem as system RAM. Or Memkind support for KMEM DAX option for more details. Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from pm to kmem as below. spark.sql.oap.cache.memory.manager kmem","title":"Guava cache"},{"location":"Advanced-Configuration/#noevict-cache","text":"The noevict cache strategy is also supported in OAP based on the memkind library for PMem. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.oap.cache.strategy noevict spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Noevict cache"},{"location":"Advanced-Configuration/#vmemcache","text":"Make sure Vmemcache library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished OAP-Installation-Guide , vmemcache library will be automatically installed by Conda. Or you can follow the build/install steps and make sure libvmemcache.so exist in /lib64 directory on each worker node. - To use it in your workload, follow prerequisites to set up PMem hardware correctly.","title":"Vmemcache"},{"location":"Advanced-Configuration/#configure-to-enable-pmem-cache","text":"Make the following configuration changes in $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable vmemcache strategy spark.oap.cache.strategy vmem spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # according to your cluster spark.executor.sql.oap.cache.guardian.memory.size 10g The vmem cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache. spark.executor.instances : We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket. spark.executor.sql.oap.cache.persistent.memory.initial.size : It is configured to the available PMem capacity to be used as data cache per exectutor. NOTE : If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set spark.executor.sql.oap.cache.guardian.memory.size to a larger number as the default size is 10GB. The user could also increase spark.sql.oap.cache.guardian.free.thread.nums or decrease spark.sql.oap.cache.dispose.timeout.ms to free memory more quickly.","title":"Configure to enable PMem cache"},{"location":"Advanced-Configuration/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Verify NUMA binding status by confirming keywords like numactl --cpubind=1 --membind=1 contained in executor launch command. Check PMem cache size by checking disk space with df -h .For vmemcache strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For Guava/Noevict strategies, the command will show disk space usage increases along with workload execution.","title":"Verify PMem cache functionality"},{"location":"Advanced-Configuration/#index-and-data-cache-separation","text":"SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio. Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to spark-defaults.conf . 1. DRAM as cache media, guava strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager offheap The rest configuration you can refer to Use DRAM Cache PMem as cache media, external strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager tmp spark.sql.oap.mix.data.cache.backend external spark.sql.oap.mix.index.cache.backend external The rest configurations can refer to the configurations of PMem Cache and External cache DRAM( offheap )/ guava as index cache media and backend, PMem( tmp )/ external as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix spark.sql.oap.mix.data.cache.backend external # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.memory.offHeap.enabled false spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true DRAM( offheap )/ guava as index cache media and backend, PMem( pm )/ guava as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 spark.memory.offHeap.enabled false # PMem capacity per executor spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true","title":"Index and Data Cache Separation"},{"location":"Advanced-Configuration/#cache-hot-tables","text":"Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2>","title":"Cache Hot Tables"},{"location":"Advanced-Configuration/#column-vector-cache","text":"This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true","title":"Column Vector Cache"},{"location":"Advanced-Configuration/#large-scale-and-heterogeneous-cluster-support","text":"NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Large Scale and Heterogeneous Cluster Support"},{"location":"Architect-Overview/","text":"Architecture Overview Introduction Usage Scenarios Architecture Features Introduction Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases. Usage Scenarios Usage Scenario 1 -- Interactive queries Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude. Usage Scenario 2 -- Batch processing jobs Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need. Architecture The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations. Features Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs. Indexing Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution. Caching Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. *Other names and brands may be claimed as the property of others.","title":"Architecture Overview"},{"location":"Architect-Overview/#architecture-overview","text":"Introduction Usage Scenarios Architecture Features","title":"Architecture Overview"},{"location":"Architect-Overview/#introduction","text":"Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.","title":"Introduction"},{"location":"Architect-Overview/#usage-scenarios","text":"","title":"Usage Scenarios"},{"location":"Architect-Overview/#usage-scenario-1-interactive-queries","text":"Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.","title":"Usage Scenario 1 -- Interactive queries"},{"location":"Architect-Overview/#usage-scenario-2-batch-processing-jobs","text":"Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need.","title":"Usage Scenario 2 -- Batch processing jobs"},{"location":"Architect-Overview/#architecture","text":"The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.","title":"Architecture"},{"location":"Architect-Overview/#features","text":"Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.","title":"Features"},{"location":"Architect-Overview/#indexing","text":"Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.","title":"Indexing"},{"location":"Architect-Overview/#caching","text":"Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.","title":"Caching"},{"location":"Architect-Overview/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"Developer-Guide/","text":"Developer Guide This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache. Building Enabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark Building Building SQL Index and Data Source Cache Building with Apache Maven* . Clone the OAP project: git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git cd OAP Build the oap-cache package: mvn clean -pl com.intel.oap:oap-cache -am package Running Tests Run all the tests: mvn clean -pl com.intel.oap:oap-cache -am test Run a specific test suite, for example OapDDLSuite : mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed. Building with Intel\u00ae Optane\u2122 DC Persistent Memory Module Prerequisites for building with PMem support Install the required packages on the build system: cmake memkind vmemcache Plasma memkind installation The memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install vmemcache installation To build vmemcache library from source, you can (for RPM-based linux as example): git clone https://github.com/pmem/vmemcache cd vmemcache mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm make package sudo rpm -i libvmemcache*.rpm Plasma installation To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-0.17.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Intel-arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-0.17.0.jar arrow-plasma-0.17.0.jar is provided in Maven central repo, you can download it and copy to $SPARK_HOME/jars dir. Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -DskipTests install Building the package You need to add -Ppersistent-memory to build with PMem support. For noevict cache strategy, you also need to build with -Ppersistent-memory parameter. mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -DskipTests package For vmemcache cache strategy, please build with command: mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package Build with this command to use all of them: mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -Pvmemcache -DskipTests package Enabling NUMA binding for PMem in Spark Rebuilding Spark packages with NUMA binding patch When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.0.0.patch to Spark source code for best performance. Download src for Spark-3.0.0 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.0.0.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true NOTE : If you are using a customized Spark, you will need to manually resolve the conflicts. *Other names and brands may be claimed as the property of others.","title":"Developer Guide"},{"location":"Developer-Guide/#developer-guide","text":"This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache. Building Enabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark","title":"Developer Guide"},{"location":"Developer-Guide/#building","text":"","title":"Building"},{"location":"Developer-Guide/#building-sql-index-and-data-source-cache","text":"Building with Apache Maven* . Clone the OAP project: git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git cd OAP Build the oap-cache package: mvn clean -pl com.intel.oap:oap-cache -am package","title":"Building SQL Index and  Data Source Cache"},{"location":"Developer-Guide/#running-tests","text":"Run all the tests: mvn clean -pl com.intel.oap:oap-cache -am test Run a specific test suite, for example OapDDLSuite : mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed.","title":"Running Tests"},{"location":"Developer-Guide/#building-with-intel-optanetm-dc-persistent-memory-module","text":"","title":"Building with Intel\u00ae Optane\u2122 DC Persistent Memory Module"},{"location":"Developer-Guide/#prerequisites-for-building-with-pmem-support","text":"Install the required packages on the build system: cmake memkind vmemcache Plasma","title":"Prerequisites for building with PMem support"},{"location":"Developer-Guide/#memkind-installation","text":"The memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install","title":"memkind installation"},{"location":"Developer-Guide/#vmemcache-installation","text":"To build vmemcache library from source, you can (for RPM-based linux as example): git clone https://github.com/pmem/vmemcache cd vmemcache mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm make package sudo rpm -i libvmemcache*.rpm","title":"vmemcache installation"},{"location":"Developer-Guide/#plasma-installation","text":"To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-0.17.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Intel-arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-0.17.0.jar arrow-plasma-0.17.0.jar is provided in Maven central repo, you can download it and copy to $SPARK_HOME/jars dir. Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -DskipTests install","title":"Plasma installation"},{"location":"Developer-Guide/#building-the-package","text":"You need to add -Ppersistent-memory to build with PMem support. For noevict cache strategy, you also need to build with -Ppersistent-memory parameter. mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -DskipTests package For vmemcache cache strategy, please build with command: mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package Build with this command to use all of them: mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -Pvmemcache -DskipTests package","title":"Building the package"},{"location":"Developer-Guide/#enabling-numa-binding-for-pmem-in-spark","text":"","title":"Enabling NUMA binding for PMem in Spark"},{"location":"Developer-Guide/#rebuilding-spark-packages-with-numa-binding-patch","text":"When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.0.0.patch to Spark source code for best performance. Download src for Spark-3.0.0 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.0.0.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true NOTE : If you are using a customized Spark, you will need to manually resolve the conflicts.","title":"Rebuilding Spark packages with NUMA binding patch"},{"location":"Developer-Guide/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git # cd OAP # sh $OAP_HOME/dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_HOME/dev/release-package . $ sh $OAP_HOME/dev/compile-oap.sh Building Specified OAP Module, such as oap-cache , run: $ sh $OAP_HOME/dev/compile-oap.sh --oap-cache Running OAP Unit Tests Setup building environment manually for intel MLlib, and if your default GCC version is before 7.0 also need export CC & CXX before using mvn , run $ export CXX=$OAP_HOME/dev/thirdparty/gcc7/bin/g++ $ export CC=$OAP_HOME/dev/thirdparty/gcc7/bin/gcc $ export ONEAPI_ROOT=/opt/intel/inteloneapi $ source /opt/intel/inteloneapi/daal/2021.1-beta07/env/vars.sh $ source /opt/intel/inteloneapi/tbb/2021.1-beta07/env/vars.sh $ source /tmp/oneCCL/build/_install/env/setvars.sh Run all the tests: $ mvn clean test Run Specified OAP Module Unit Test, such as oap-cache : $ mvn clean -pl com.intel.oap:oap-cache -am test Building SQL Index and Data Source Cache with PMem Prerequisites for building with PMem support When using SQL Index and Data Source Cache with PMem, finish steps of Prerequisites for building to ensure needed dependencies have been installed. Building package You can build OAP with PMem support with command below: $ sh $OAP_HOME/dev/compile-oap.sh Or run: $ mvn clean -q -Ppersistent-memory -Pvmemcache -DskipTests package Contributing This session introduces what is required before submitting a code change to OAP. We continue to use the Github Issues to track the new features/tasks/issues.\u200b For every commit, we need an issue id for the commit. \u200b Format the log message as following: [OAP-IssuesId][optional:ModuleName] detailed message \u200b like [OAP-1406][rpmem-shuffle]Add shuffle block removing operation within one Spark context Always merge your pull request as a single commit and the commit message follow the above format.\u200b The formal features names in 0.9 are: SQL Index , SQL Data Source Cache , Native SQL Engine , Unified Arrow Data Source , RDD Cache PMem Extension , RPMem Shuffle , Remote Shuffle , Intel MLlib . We don\u2019t strictly request the module id the same as the feature name. Please align in the feature members to use a consistent name in the log message.\u200b","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git # cd OAP # sh $OAP_HOME/dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_HOME/dev/release-package . $ sh $OAP_HOME/dev/compile-oap.sh Building Specified OAP Module, such as oap-cache , run: $ sh $OAP_HOME/dev/compile-oap.sh --oap-cache","title":"Building"},{"location":"OAP-Developer-Guide/#running-oap-unit-tests","text":"Setup building environment manually for intel MLlib, and if your default GCC version is before 7.0 also need export CC & CXX before using mvn , run $ export CXX=$OAP_HOME/dev/thirdparty/gcc7/bin/g++ $ export CC=$OAP_HOME/dev/thirdparty/gcc7/bin/gcc $ export ONEAPI_ROOT=/opt/intel/inteloneapi $ source /opt/intel/inteloneapi/daal/2021.1-beta07/env/vars.sh $ source /opt/intel/inteloneapi/tbb/2021.1-beta07/env/vars.sh $ source /tmp/oneCCL/build/_install/env/setvars.sh Run all the tests: $ mvn clean test Run Specified OAP Module Unit Test, such as oap-cache : $ mvn clean -pl com.intel.oap:oap-cache -am test","title":"Running OAP Unit Tests"},{"location":"OAP-Developer-Guide/#building-sql-index-and-data-source-cache-with-pmem","text":"","title":"Building SQL Index and Data Source Cache with PMem"},{"location":"OAP-Developer-Guide/#prerequisites-for-building-with-pmem-support","text":"When using SQL Index and Data Source Cache with PMem, finish steps of Prerequisites for building to ensure needed dependencies have been installed.","title":"Prerequisites for building with PMem support"},{"location":"OAP-Developer-Guide/#building-package","text":"You can build OAP with PMem support with command below: $ sh $OAP_HOME/dev/compile-oap.sh Or run: $ mvn clean -q -Ppersistent-memory -Pvmemcache -DskipTests package","title":"Building package"},{"location":"OAP-Developer-Guide/#contributing","text":"This session introduces what is required before submitting a code change to OAP. We continue to use the Github Issues to track the new features/tasks/issues.\u200b For every commit, we need an issue id for the commit. \u200b Format the log message as following: [OAP-IssuesId][optional:ModuleName] detailed message \u200b like [OAP-1406][rpmem-shuffle]Add shuffle block removing operation within one Spark context Always merge your pull request as a single commit and the commit message follow the above format.\u200b The formal features names in 0.9 are: SQL Index , SQL Data Source Cache , Native SQL Engine , Unified Arrow Data Source , RDD Cache PMem Extension , RPMem Shuffle , Remote Shuffle , Intel MLlib . We don\u2019t strictly request the module id the same as the feature name. Please align in the feature members to use a consistent name in the log message.\u200b","title":"Contributing"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration Prerequisites SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. Getting Started Building We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps. Spark Configurations Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Verify Integration After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run. Configuration for YARN Cluster Mode Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar Configuration for Spark Standalone Mode In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Working with SQL Index After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") Index Creation Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show() Use Index Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show() Drop index Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\") Working with SQL Data Source Cache Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware. Use DRAM Cache Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example. Use PMem Cache Prerequisites The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload. Configuration for NUMA Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark . Configuration for enabling PMem cache Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h . Run TPC-DS Benchmark This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly. Prerequisites Python 2.7+ is required on the working node. Prepare the Tool Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ). Generate TPC-DS Data Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables. Start Spark Thrift Server Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server. Use PMem as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache. Use DRAM as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start Run Queries Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5. Advanced Configuration Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . - Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. - Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. - Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. - Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"User Guide"},{"location":"User-Guide/#user-guide","text":"Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration","title":"User Guide"},{"location":"User-Guide/#prerequisites","text":"SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.","title":"Prerequisites"},{"location":"User-Guide/#getting-started","text":"","title":"Getting Started"},{"location":"User-Guide/#building","text":"We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps.","title":"Building"},{"location":"User-Guide/#spark-configurations","text":"Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Spark Configurations"},{"location":"User-Guide/#verify-integration","text":"After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run.","title":"Verify Integration"},{"location":"User-Guide/#configuration-for-yarn-cluster-mode","text":"Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar","title":"Configuration for YARN Cluster Mode"},{"location":"User-Guide/#configuration-for-spark-standalone-mode","text":"In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Configuration for Spark Standalone Mode"},{"location":"User-Guide/#working-with-sql-index","text":"After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\")","title":"Working with SQL Index"},{"location":"User-Guide/#index-creation","text":"Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show()","title":"Index Creation"},{"location":"User-Guide/#use-index","text":"Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show()","title":"Use Index"},{"location":"User-Guide/#drop-index","text":"Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\")","title":"Drop index"},{"location":"User-Guide/#working-with-sql-data-source-cache","text":"Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.","title":"Working with SQL Data Source Cache"},{"location":"User-Guide/#use-dram-cache","text":"Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.","title":"Use DRAM Cache"},{"location":"User-Guide/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"User-Guide/#prerequisites_1","text":"The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.","title":"Prerequisites"},{"location":"User-Guide/#configuration-for-numa","text":"Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark .","title":"Configuration for NUMA"},{"location":"User-Guide/#configuration-for-enabling-pmem-cache","text":"Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it.","title":"Configuration for enabling PMem cache"},{"location":"User-Guide/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h .","title":"Verify PMem cache functionality"},{"location":"User-Guide/#run-tpc-ds-benchmark","text":"This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.","title":"Run TPC-DS Benchmark"},{"location":"User-Guide/#prerequisites_2","text":"Python 2.7+ is required on the working node.","title":"Prerequisites"},{"location":"User-Guide/#prepare-the-tool","text":"Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ).","title":"Prepare the Tool"},{"location":"User-Guide/#generate-tpc-ds-data","text":"Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables.","title":"Generate TPC-DS Data"},{"location":"User-Guide/#start-spark-thrift-server","text":"Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.","title":"Start Spark Thrift Server"},{"location":"User-Guide/#use-pmem-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache.","title":"Use PMem as Cache Media"},{"location":"User-Guide/#use-dram-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start","title":"Use DRAM as Cache Media"},{"location":"User-Guide/#run-queries","text":"Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5.","title":"Run Queries"},{"location":"User-Guide/#advanced-configuration","text":"Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . - Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. - Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. - Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. - Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"Advanced Configuration"}]}